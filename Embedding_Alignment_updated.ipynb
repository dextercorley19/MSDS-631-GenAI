{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üß† **Learning Joint Image‚ÄìText Embeddings with Contrastive Loss (Flickr8k + GloVe)**\n",
        "\n",
        "#### **Goal**\n",
        "\n",
        "We want to train a model that learns a **shared embedding space** where:\n",
        "\n",
        "* The embedding of an **image** and the embedding of its **caption** are **close**.\n",
        "* Embeddings of **non-matching** image‚Äìcaption pairs are **far apart**.\n",
        "\n",
        "This enables tasks like:\n",
        "\n",
        "* Retrieving images given text (and vice versa)\n",
        "* Cross-modal understanding\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ **Mathematical Objective**\n",
        "\n",
        "Let:\n",
        "\n",
        "* $x_i \\in \\mathbb{R}^d$: embedding of the **$i$th caption**\n",
        "* $y_i \\in \\mathbb{R}^d$: embedding of the **$i$th image**\n",
        "\n",
        "We want to **maximize similarity** between $x_i$ and $y_i$\n",
        "and **minimize similarity** between $x_i$ and all $y_j$, $j \\ne i$.\n",
        "\n",
        "We define a **similarity matrix** $S \\in \\mathbb{R}^{N \\times N}$ for a batch of size $N$:\n",
        "\n",
        "$$\n",
        "S_{ij} = \\langle x_i, y_j \\rangle\n",
        "$$\n",
        "\n",
        "This is the dot product (cosine similarity if vectors are normalized) between caption $i$ and image $j$.\n",
        "\n",
        "We then use **cross-entropy loss** to encourage:\n",
        "\n",
        "* $S_{ii}$ to be high (correct match)\n",
        "* $S_{ij}$ for $j \\ne i$ to be low (distractors)\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} -\\log \\left( \\frac{\\exp(S_{ii})}{\\sum_{j=1}^{N} \\exp(S_{ij})} \\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üß© **Model Components**\n",
        "\n",
        "* **Text Encoder**: Takes the GloVe vectors of all words in a caption and averages them to a 300-dim vector, then projects to 128-dim.\n",
        "* **Image Encoder**: A pretrained ResNet-50 with its final layer replaced to output 128-dim embeddings.\n",
        "* Both outputs are **L2-normalized** to lie on a unit hypersphere.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Training Summary**\n",
        "\n",
        "* Data: Flickr8k images and first caption per image\n",
        "* Loss: Contrastive loss over dot-product similarity\n",
        "* Optimizer: Adam\n",
        "* Output: A model that embeds matching image‚Äìcaption pairs closely\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Why This Works**\n",
        "\n",
        "This method is inspired by **CLIP** (Contrastive Language‚ÄìImage Pretraining), where aligning images and natural language in a shared vector space enables powerful multimodal tasks‚Äîeven with simple linear classifiers or nearest-neighbor search.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfnl4JUMxAL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Constants\n",
        "EMBED_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Download GloVe and Flickr8k datasets\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip Flickr8k_Dataset.zip\n",
        "!unzip Flickr8k_text.zip\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(filepath):\n",
        "    embeddings_index = {}\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "glove_path = 'glove.6B.300d.txt'\n",
        "embeddings_index = load_glove_embeddings(glove_path)\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_caption(caption):\n",
        "    tokens = word_tokenize(caption.lower())\n",
        "    return [embeddings_index[token] for token in tokens if token in embeddings_index]\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, image_folder, captions_file, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.captions = self.load_captions(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def load_captions(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        captions = {}\n",
        "        for line in lines:\n",
        "            tokens = line.split('\\t')\n",
        "            image_id, caption = tokens[0], tokens[1]\n",
        "            if image_id not in captions:\n",
        "                captions[image_id] = []\n",
        "            captions[image_id].append(caption)\n",
        "        return captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = list(self.captions.keys())[idx]\n",
        "        caption = self.captions[image_id][0]  # Use the first caption\n",
        "        image_path = os.path.join(self.image_folder, image_id)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        caption_embedding = preprocess_caption(caption)\n",
        "        return image, torch.tensor(caption_embedding, dtype=torch.float)\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "image_folder = 'Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "captions_file = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "dataset = FlickrDataset(image_folder, captions_file, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Define the Model\n",
        "class SimpleAlignModel(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SimpleAlignModel, self).__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
        "        self.text_fc = nn.Linear(300, embed_dim)\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        text_embed = F.normalize(self.text_fc(text), dim=-1)\n",
        "        image_embed = F.normalize(self.resnet(image), dim=-1)\n",
        "        return text_embed, image_embed\n",
        "\n",
        "# Initialize Model\n",
        "model = SimpleAlignModel(embed_dim=EMBED_DIM)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, captions in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        # Flatten the caption embeddings to (batch_size, 300)\n",
        "        captions = captions.view(batch_size, -1, 300).mean(dim=1)\n",
        "\n",
        "        text_embed, image_embed = model(captions, images)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        similarity_matrix = torch.matmul(text_embed, image_embed.t())\n",
        "\n",
        "        # Compute loss (contrastive loss using labels as ground truth)\n",
        "        labels = torch.arange(similarity_matrix.size(0)).to(similarity_matrix.device)\n",
        "        loss = criterion(similarity_matrix, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {total_loss / len(dataloader):.4f}')\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "id": "Ev11_9rkw8N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UGotZYWNw-Se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "1. **Data Preparation:**\n",
        "   - **GloVe Embeddings:** We download and load the GloVe word embeddings.\n",
        "   - **Flickr8k Dataset:** We download and unzip the Flickr8k dataset.\n",
        "   - **Preprocessing:** Captions are tokenized and converted to embeddings using GloVe. Images are transformed using standard preprocessing steps.\n",
        "\n",
        "2. **Model Definition:**\n",
        "   - **ResNet:** We use a pretrained ResNet50 model to extract image embeddings, modifying the final layer to output the desired embedding dimension.\n",
        "   - **Text Embeddings:** A simple linear layer maps the GloVe embeddings to the common embedding space.\n",
        "\n",
        "3. **Training Loop:**\n",
        "   - **Forward Pass:** We compute embeddings for both text and images.\n",
        "   - **Similarity Matrix:** A similarity matrix is computed using the dot product of text and image embeddings.\n",
        "   - **Loss Calculation:** We use cross-entropy loss to align the embeddings.\n",
        "   - **Backpropagation:** The loss is backpropagated, and the model parameters are updated.\n",
        "\n"
      ],
      "metadata": {
        "id": "oQRM0DT-5flt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tINlW6N55rNf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}