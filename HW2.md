# Homework: Fine-Tuning a Pre-trained Model on the Butterfly Dataset

## Objective

In this homework, you will fine-tune a pre-trained image generation model using the butterfly dataset. The goal is to understand the process of fine-tuning and observe how the model adapts to a new dataset.

You will mainly use the following hugging face note books :
- [Introduction to diffusion models](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit1/01_introduction_to_diffusers.ipynb)
- [Fine-Tuning Guide on GitHub](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/01_finetuning_and_guidance.ipynb)

## Instructions

### 1. Load and Prepare the Dataset

- Download the butterfly dataset. 
- Create a dataloader to sample a batch of images.
- Preprocess the images by resizing them, applying random horizontal flips, converting to tensors, and normalizing.

### 2. Fine-Tune the Model

- Set the optimization target to `image_pipe.unet.parameters()`.
- Create a training loop to update the model weights using the preprocessed dataset.
- Implement gradient accumulation to handle small batch sizes and GPU memory constraints.

### 3. Considerations

- **Batch Size**: Use a small batch size due to GPU memory constraints. You can adjust the image size to speed up training or allow for larger batches.
- **Gradient Accumulation**: Accumulate gradients over several batches before updating the model parameters. This simulates a larger batch size and helps in stabilizing the training process.
- **Logging and Monitoring**: Generate samples during training to visually inspect the model's progress. Use logging tools like Weights and Biases (wandb) or TensorBoard to track training metrics.

### 4. Optional: Logging with Weights and Biases

- Initialize a wandb project and log loss values during training. Wandb is not considered defeault knowledge in industry for ML engineers working in DL.
- Track the average loss per epoch for better monitoring.

## Submission

Submit a single Jupyter Notebook file (.ipynb) with the following:

1. **Code Implementation**: The complete code used to fine-tune the model, including:
   - Loading the dataset
   - Preprocessing
   - Setting up the training loop
   - Applying gradient accumulation

2. **Training Logs**: Logs showing the training progress, including loss values per epoch. If using wandb, include a link to your project dashboard.

3. **Generated Samples**: A few images generated by the model at different stages of training (e.g., after each epoch).

## Resources

For detailed code and step-by-step instructions, refer to the [Fine-Tuning Guide on GitHub](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/01_finetuning_and_guidance.ipynb). This notebook contains the complete code and explanations needed to fine-tune the model on the butterfly dataset and monitor its performance.

By completing this homework, you will gain practical experience in fine-tuning a pre-trained image generation model and understanding the considerations involved in adapting models to new datasets. Good luck and have fun experimenting!
